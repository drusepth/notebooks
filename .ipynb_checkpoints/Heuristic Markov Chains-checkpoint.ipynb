{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "This notebook details a new approach to Markov chain generation that uses heuristics to guide generation towards an ideal full-text metric.\n",
    "\n",
    "Markov chains are good at generating random human-like text. They are bad at higher-level guidance and heuristics on that text generation. \n",
    "\n",
    "This algorithm aims to add a higher level heuristic function to Markov chain generation to allow for additional assistance guiding Markov chains toward a more-desirable chain output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional frequency-based Markov chain generation\n",
    "\n",
    "Currently, Markov chains employ a selection function to choose what word follows each prior word. In its simplest form, this function often takes the form of frequency-based selection. That is, if \"are\" most-commonly follows the word \"The\", it has the highest chance of being selected.\n",
    "\n",
    "Pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_words = [some_initial_word]\n",
    "# while not complete_sentence(chain_words)\n",
    "#   current_word = chain_words[-1]\n",
    "#     \n",
    "#   potential_followup_words = followup_words_for(current_word)\n",
    "# \n",
    "#   ranked_followup_words = potential_followup_words.sort(function (word) {\n",
    "#       # Rank words by the frequency they occur after `word`, so we can select the most-frequent one easily\n",
    "#       frequency = frequency_function(chain_words[-1], word)\n",
    "#   })\n",
    "#     \n",
    "#   selected_word = ranked_followup_words.first\n",
    "#   chain_words << selected_word\n",
    "#   current_word = selected_word\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic-driven Markov chain generation\n",
    "\n",
    "Instead of always defaulting to a frequency-driven word selection, we can define a generic heuristic to guide which word should be selected at each chain node. Given a function `heuristic_function(option_one, option_two)` that returns which of two options is closer to the desired heuristic ideal, the Markov generation code can be modified to rank potential words based on their closeness to the heuristic ideal.\n",
    "\n",
    "Pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_words = [some_initial_word]\n",
    "# while not complete_sentence(chain_words)\n",
    "#   current_word = chain_words[-1]\n",
    "#     \n",
    "#   potential_followup_words = followup_words_for(current_word)\n",
    "# \n",
    "#   ranked_followup_words = potential_followup_words.sort(function (word) {\n",
    "#       heuristic_fitness = heuristic_function(chain_words + [word])\n",
    "#   })\n",
    "#     \n",
    "#   selected_word = ranked_followup_words.first\n",
    "#   chain_words << selected_word\n",
    "#   current_word = selected_word\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, both approaches (frequency-based selection by n-gram, as well as heuristic-driven selection of the entire sentence) can be generalized into the same approach, if you consider frequency selection as simply another heuristic: the output is considered \"better\" if each word often follows the previous word in other sentences. \n",
    "\n",
    "The parameters of `heuristic_function(sentence_words_array)` differ in arity, and can be further generalized to `*args`. To that end, we result in the following Markov chain generation method (in Python 3.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def markov_chain(initial_word=None, heuristic_function=lambda x, y: 0):\n",
    "    chain_words = [initial_word]\n",
    "   \n",
    "    while chain_words[-1] != None:\n",
    "        current_word = chain_words[-1]\n",
    "        \n",
    "        # Generate a pool of potential consequents to choose from\n",
    "        potential_followup_words = followup_words_for(current_word)\n",
    "        \n",
    "        # Rank that pool of consequents by a heuristic comparing options and choose the one with the highest score\n",
    "        highest_score = -1\n",
    "        for word in potential_followup_words:\n",
    "            heuristic_score = heuristic_function(chain_words, word)\n",
    "            if (heuristic_function(chain_words, word) > highest_score):\n",
    "                selected_word = word\n",
    "                highest_score = heuristic_score\n",
    "        \n",
    "        # Choose a word and advance forward through the chain\n",
    "        chain_words.append(selected_word)\n",
    "    \n",
    "    return \" \".join(chain_words[0:-1]) + \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a few ancillary methods to help here: `followup_words_for(current_word)` and, of course, `heuristic_function`. We'll tackle the latter in just a moment, but for the sake of demonstration we'll include a mocked API response for the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followup_words_for(current_word):\n",
    "    sanitized_word = current_word.lower()\n",
    "    \n",
    "    if (sanitized_word == \"there\"):\n",
    "        return [\"is\", \"isn't\"]\n",
    "    elif (sanitized_word == \"is\" or sanitized_word == \"isn't\"):\n",
    "        return [\"no\", \"a\"]\n",
    "    elif (sanitized_word == \"no\" or sanitized_word == \"a\"):\n",
    "        return [\"cow\", \"dog\", \"elephant\"]\n",
    "    elif (sanitized_word == \"cow\" or sanitized_word == \"dog\" or sanitized_word == \"elephant\"):\n",
    "        return [\"level\"]\n",
    "    elif (sanitized_word == \"level\"):\n",
    "        return [None]\n",
    "    else:\n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mocked response will return the string \"There is no cow level\" when initiating `markov_chain()` with `initial_word=There`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no cow level.\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain(initial_word=\"There\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this format, we can replicate the traditional frequency-based word selection with a simple heuristic selecting for words that occur most-often. To accomplish this, we'll also include a separate frequency-lookup service for an n-gram pair (also mocked up here in the name of simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_of(antecedent, consequent):\n",
    "    if (antecedent == \"there\"):\n",
    "        if (consequent == \"is\"):\n",
    "            return 100\n",
    "        elif (consequent == \"isn't\"):\n",
    "            return 1\n",
    "    \n",
    "    if (antecedent == \"no\"):\n",
    "        if (consequent == \"cow\"):\n",
    "            return 1\n",
    "        elif (consequent == \"dog\"):\n",
    "            return 100\n",
    "        elif (consequent == \"elephant\"):\n",
    "            return 1\n",
    "    \n",
    "    # catchall\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_heuristic(existing_chain, new_word):\n",
    "    previous_word = existing_chain[-1]\n",
    "    \n",
    "    # Return the frequency of new_word appearing after previous_word\n",
    "    return frequency_of(previous_word, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no dog level.\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain(initial_word=\"There\", heuristic_function=frequency_heuristic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, other heuristics can be defined. For example, selecting the _longest_ potential word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_word_heuristic(existing_chain, new_word):\n",
    "    if new_word == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There isn't no elephant level.\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain(initial_word=\"There\", heuristic_function=longest_word_heuristic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, of course, the _shortest_ possible word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_word_heuristic(existing_chain, new_word):\n",
    "    if new_word == None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / len(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a cow level.\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain(initial_word=\"There\", heuristic_function=shortest_word_heuristic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications of this heuristic-based approach to node selection are most notable in the field of identity mirroring, guiding text that was previously randomly generated to align closer with objective metrics that exist a measurable distance from desired values. For example, generating text specifically at a 6th grade reading level, or in a particular dialect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, Markov chains can be trained on particular actors or characters, sourcing potential words from their vocabulary and use and then further refined in node selection with their linguistic style.\n",
    "\n",
    "Before getting into some examples, lets expand the (mocked) API responses for n-gram pairs so we can generate a larger variety of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followup_words_for(current_word):\n",
    "    sanitized_word = current_word.lower()\n",
    "    \n",
    "    if (sanitized_word == \"the\"):\n",
    "        return [\"quick\", \"fox\"]\n",
    "    elif (sanitized_word == \"quick\"):\n",
    "        return [\"brown\", \"red\"]\n",
    "    elif (sanitized_word in [\"brown\", \"red\"]):\n",
    "        return [\"fox\", \"elephant\"]\n",
    "    elif (sanitized_word in [\"fox\", \"elephant\"]):\n",
    "        return [\"jumps\"]\n",
    "    elif (sanitized_word == \"jumps\"):\n",
    "        return [\"over\", None]\n",
    "    elif (sanitized_word == \"over\"):\n",
    "        return [\"logs\", \"decaying\"]\n",
    "    elif (sanitized_word == \"decaying\"):\n",
    "        return [\"logs\"]\n",
    "    else:\n",
    "        return [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Bob the Blacksmith\n",
    "\n",
    "Consider Bob the Blacksmith. He speaks succinctly. He's direct. His style is distinct. You recognize his style. It's stoccato. \n",
    "\n",
    "Bob's speech could be modeled in a number of different ways, but one comes to mind: sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_sentence_heuristic(existing_chain, new_word):\n",
    "    if new_word == None:\n",
    "        return len(existing_chain)\n",
    "    else:\n",
    "        return len(existing_chain) + 1\n",
    "\n",
    "def shortest_sentence_heuristic(existing_chain, new_word):\n",
    "    return 1 / longest_sentence_heuristic(existing_chain, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over logs.\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain(initial_word=\"The\", heuristic_function=longest_sentence_heuristic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps.\n"
     ]
    }
   ],
   "source": [
    "print(markov_chain(initial_word=\"The\", heuristic_function=shortest_sentence_heuristic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
